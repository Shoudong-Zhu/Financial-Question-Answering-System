# Financial Question Answering with Mistral-7B-v0.1

## Overview

This project aims to build an AI model to accurately answer financial questions with numerical or categorical answers. The model leverages the Mistral-7B-v0.1 pre-trained model, fine-tuned on the RAG Instruct Benchmark Tester dataset. The final model generates answers to questions from the Bizbench testing dataset.

## Files in the Repository

1. **main.py**: The main script containing the implementation for loading data, fine-tuning the model, and generating answers.
2. **output.csv**: The output file containing the answers generated by the model for the Bizbench testing dataset. The format is a CSV file with two columns: `id` and `answer`.
3. **README.md**: This file, detailing the technical approach and instructions for the project.

## Technical Approach

### Data Preparation

1. **Loading Data**: The data from the Bizbench testing dataset is loaded and prepared for processing.
2. **Tokenization**: The dataset is tokenized using the `AutoTokenizer` from the Hugging Face library. A new padding token is added to ensure consistent input sizes.

### Model Preparation

1. **Model Loading**: The Mistral-7B-v0.1 model is loaded using the `AutoModelForCausalLM` from Hugging Face.
2. **Fine-Tuning**: The model is fine-tuned on the RAG Instruct Benchmark Tester dataset, which is split into training and validation sets. Tokenized inputs are prepared, and the model is fine-tuned using the `Trainer` class from Hugging Face with appropriate training arguments.

### Answer Generation

1. **Generating Answers**: The model generates answers for the questions in the Bizbench testing dataset. The answers are either numerical or the index of the correct multiple-choice option.
2. **Error Handling**: Runtime and unexpected errors during generation are handled, ensuring that the process continues smoothly for other inputs.

## Dependencies

- `torch`
- `transformers`
- `datasets`
- `retry`
- `logging`
- `pandas`

## Reflection and Potential Improvements

### Approach

The approach involved fine-tuning the Mistral-7B-v0.1 model on a financial dataset and using it to generate answers for the Bizbench testing dataset. The fine-tuning process aimed to adapt the model to understand and generate appropriate responses to financial questions.

### Model and Parameters

- **Model**: Mistral-7B-v0.1
- **Learning Rate**: 2e-5
- **Batch Size**: 4 (for both training and evaluation)
- **Number of Epochs**: 10
- **Weight Decay**: 0.01
- **Max Length for Tokenization**: 1024

## Reflection and Potential Improvements

### Observations

1. **Incorrect Numerical Answers**: Many answers generated were incorrect numerically. This indicates that the model might not fully understand the context or the computation required. This could be due to the model's limitations in performing precise arithmetic operations or interpreting complex financial data correctly.
2. **None Values**: Some answers were `None`, which suggests that the model either failed to generate a response or produced an invalid output. This might be due to the model not handling certain edge cases well or the input data being improperly formatted or understood.

### Potential Improvements

1. **Data Quality and Quantity**: Increasing the size and diversity of the training dataset could help the model learn better. More examples, especially those covering edge cases and diverse financial scenarios, can improve the model's robustness and accuracy.
2. **Contextual Understanding**: Incorporating additional context or using more advanced techniques to provide context might improve the model's understanding. This could involve better preprocessing of input data, using external knowledge bases, or applying techniques like attention mechanisms to focus on relevant parts of the input.
3. **Model Architecture**: Experimenting with different model architectures or ensembles could yield better results. Models specifically designed for numerical reasoning or financial data, or combinations of multiple models, might improve performance.
4. **Hyperparameter Tuning**: Further tuning the hyperparameters such as learning rate, batch size, and number of epochs could enhance model performance. Systematic experimentation with different hyperparameter values might lead to more optimal training conditions.
5. **Error Analysis**: Conducting a thorough error analysis to understand why certain answers were incorrect can provide insights for targeted improvements. This involves analyzing the types of errors made, identifying common patterns, and devising strategies to address these issues.
6. **Improved Fine-Tuning**: Given more time, fine-tuning the model on a more specific financial dataset that includes a variety of numerical questions and answers could improve accuracy. This involves curating a dataset that closely mirrors the types of questions expected in real-world applications.
7. **Advanced Techniques**: Implementing advanced techniques such as reinforcement learning, self-training, or using auxiliary tasks to guide the model's learning process might improve its performance.